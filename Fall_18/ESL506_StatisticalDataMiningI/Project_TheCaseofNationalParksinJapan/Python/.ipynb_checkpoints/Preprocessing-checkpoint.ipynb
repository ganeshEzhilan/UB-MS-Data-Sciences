{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from google.cloud import translate\n",
    "translate_client = translate.Client()\n",
    "path = '/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/Signate-4d738aae3eb1.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "\n",
    "def translate_eng(text, target='en'):\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "    return result['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating training set\n",
    "train = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'train.tsv'.txt\")\n",
    "park_trans = pd.DataFrame()\n",
    "park_trans['park'] = list(set(train['park']))\n",
    "e_park = []\n",
    "for park in park_trans['park']:\n",
    "    if park.strip is not None:\n",
    "        try:\n",
    "            e_park.append(translator.translate(park).text)\n",
    "        except:\n",
    "            pass\n",
    "park_trans['epark'] = e_park\n",
    "t_train = train.merge(park_trans, on='park')[['datetime', 'epark', 'visitors']]\n",
    "t_train.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'ttrain.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating test set\n",
    "test = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'test.tsv'.txt\")\n",
    "t_test = test.merge(park_trans, on='park')[['index','datetime', 'epark',]]\n",
    "t_test = t_test.sort_values(by = ['index'])\n",
    "t_test.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'ttest.tsv'.txt\", sep='\\t', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating hotlink\n",
    "hotlink = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'hotlink.tsv'.txt\", sep='\\t')\n",
    "hotlink_trans = pd.DataFrame()\n",
    "hotlink_trans['keyword'] = list(set(hotlink['keyword']))\n",
    "e_keyword = []\n",
    "for word in hotlink_trans['keyword']:\n",
    "    if word.strip is not None:\n",
    "        try:\n",
    "            e_keyword.append(translator.translate(word).text)\n",
    "        except:\n",
    "            pass\n",
    "hotlink_trans['ekeyword'] = e_keyword\n",
    "t_hotlink = hotlink.merge(hotlink_trans, on='keyword')[['datetime', 'domain', 'ekeyword','count']]\n",
    "t_hotlink = t_hotlink.sort_values(by = ['datetime'])\n",
    "t_hotlink.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'thotlink.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating colopl\n",
    "colopl = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'colopl.tsv'.txt\", sep='\\t')\n",
    "colopl_trans = pd.DataFrame()\n",
    "colopl_trans['country_jp'] = list(set(colopl['country_jp']))\n",
    "e_country_jp = []\n",
    "for word in colopl_trans['country_jp']:\n",
    "    if word.strip is not None:\n",
    "        try:\n",
    "            e_country_jp.append(translator.translate(word).text)\n",
    "        except:\n",
    "            pass\n",
    "colopl_trans['ecountry_jp'] = e_country_jp\n",
    "t_colopl = colopl.merge(park_trans, on='park')[['year', 'month', 'epark','country_jp','count']]\n",
    "t_colopl = t_colopl.merge(colopl_trans, on='country_jp')[['year', 'month', 'epark','ecountry_jp','count']]\n",
    "t_colopl = t_colopl.sort_values(by = ['year','month'])\n",
    "t_colopl.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'tcolopl.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating weather\n",
    "weather = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'weather.tsv'.txt\", sep='\\t')\n",
    "\n",
    "#translating column names\n",
    "weatherCol_trans = pd.DataFrame()\n",
    "weatherCol_trans['col'] = list(weather.columns)\n",
    "e_wcol = []\n",
    "for col in weatherCol_trans['col']:\n",
    "    if col.strip is not None:\n",
    "        try:\n",
    "            e_wcol.append(translate_eng(col))\n",
    "        except:\n",
    "            pass\n",
    "weatherCol_trans['ecol'] = e_wcol\n",
    "t_weather = weather.copy()\n",
    "t_weather.columns = e_wcol\n",
    "\n",
    "#translating location \n",
    "weatherloc_trans = pd.DataFrame()\n",
    "weatherloc_trans['location'] = list(set(weather.iloc[:,1]))\n",
    "e_weatherloc = []\n",
    "for loc in weatherloc_trans['location']:\n",
    "    if loc.strip is not None:\n",
    "        try:\n",
    "            e_weatherloc.append(translate_eng(loc))\n",
    "        except:\n",
    "            pass\n",
    "weatherloc_trans['elocation'] = e_weatherloc\n",
    "t_weather = t_weather.merge(weatherloc_trans, on='location')\n",
    "t_weather = t_weather.drop(\"location\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating column 14-\"Weather overview (night: 18 o'clock - next day 06 o'clock)\"\n",
    "weatherover_trans = pd.DataFrame()\n",
    "weatherover_trans[\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\"] = list(set(weather.iloc[:,23]))\n",
    "e_weatherover = []\n",
    "for tmp in weatherover_trans[\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\"]:\n",
    "        try:\n",
    "            e_weatherover.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weatherover.append(\"\")\n",
    "weatherover_trans[\"eWeather overview (night: 18 o'clock - next day 06 o'clock)\"] = e_weatherover\n",
    "t_weather = t_weather.merge(weatherover_trans, on=\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\")\n",
    "t_weather = t_weather.drop(\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\", axis = 1)\n",
    "\n",
    "#########\n",
    "weather_winddir = pd.DataFrame()\n",
    "weather_winddir[\"Most wind direction (16 directions)\"] = list(set(weather.iloc[:,14]))\n",
    "e_weather_winddir = []\n",
    "for tmp in weather_winddir[\"Most wind direction (16 directions)\"]:\n",
    "        try:\n",
    "            e_weather_winddir.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weather_winddir.append(\"\")\n",
    "weather_winddir[\"eMost wind direction (16 directions)\"] = e_weather_winddir\n",
    "t_weather = t_weather.merge(weather_winddir, on=\"Most wind direction (16 directions)\")\n",
    "t_weather = t_weather.drop([\"Most wind direction (16 directions)\"], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "weatherover_trans = pd.DataFrame()\n",
    "weatherover_trans[\"Weather overview (day: 06: 00-18: 00)\"] = list(set(weather.iloc[:,22]))\n",
    "e_weatherover = []\n",
    "for tmp in weatherover_trans[\"Weather overview (day: 06: 00-18: 00)\"]:\n",
    "        try:\n",
    "            e_weatherover.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weatherover.append(\"\")\n",
    "weatherover_trans[\"eWeather overview (day: 06: 00-18: 00)\"] = e_weatherover\n",
    "t_weather = t_weather.merge(weatherover_trans, on=\"Weather overview (day: 06: 00-18: 00)\")\n",
    "t_weather = t_weather.drop(\"Weather overview (day: 06: 00-18: 00)\", axis = 1)\n",
    "t_weather.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'tweather.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating files in agoop folder\n",
    "#CITY_*.tsv\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "file_list = os.listdir(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop\")\n",
    "city_list = list(filter(lambda x: x.startswith('city'), file_list)) \n",
    "mesh_list = list(filter(lambda x: x.startswith('mesh'), file_list))\n",
    "month_list = list(filter(lambda x: x.startswith('month'), file_list))\n",
    "\n",
    "\n",
    "for file in city_list:\n",
    "    city = pd.read_table(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop/\",file), sep='\\t')\n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['prefecture'] = list(set(city.iloc[:,1]))\n",
    "    e_prefecture = []\n",
    "    for prefecture in tmpdf['prefecture']:\n",
    "        if prefecture is not None:\n",
    "            trans = translate_eng(prefecture)\n",
    "            re_comp = re.compile(re.escape('prefecture'), re.IGNORECASE)\n",
    "            trans = re_comp.sub(\"\",trans)\n",
    "            e_prefecture.append(trans.strip())\n",
    "    tmpdf['eprefecture'] = e_prefecture\n",
    "    t_city = city.merge(tmpdf, on='prefecture')\n",
    "    t_city = t_city.drop([\"prefecture\"], axis = 1)\n",
    "    \n",
    "    \n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['cwtv'] = list(set(city.iloc[:,3]))\n",
    "    e_cwtv = []\n",
    "    for cwtv in tmpdf['cwtv']:\n",
    "        if type(cwtv) is str:\n",
    "            trans = translate_eng(cwtv)\n",
    "            e_cwtv.append(trans.strip())\n",
    "        else:\n",
    "            e_cwtv.append(\"\")\n",
    "    tmpdf['ecwtv'] = e_cwtv\n",
    "    t_city = t_city.merge(tmpdf, on='cwtv')\n",
    "    t_city = t_city.drop([\"cwtv\"], axis = 1)\n",
    "    \n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['address'] = list(set(city.iloc[:,4]))\n",
    "    e_add = []\n",
    "    for add in tmpdf['address']:\n",
    "        if type(add) is str:\n",
    "            trans = translate_eng(add)\n",
    "            e_add.append(trans.strip())\n",
    "        else:\n",
    "            e_add.append(\"\")\n",
    "    tmpdf['eaddress'] = e_add\n",
    "    t_city = t_city.merge(tmpdf, on='address')\n",
    "    t_city = t_city.drop([\"address\"], axis = 1)\n",
    "    t_city.to_csv(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/tagoop/\",file), sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Translating Agoop/month** files\n",
    "for file in month_list:\n",
    "    month_time = pd.read_table(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop/\",file), sep='\\t')\n",
    "    month_time = month_time.merge(park_trans, on='park')\n",
    "    month_time = month_time.drop([\"park\"], axis = 1)\n",
    "    month_time.to_csv(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/tagoop/\",file), sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park</th>\n",
       "      <th>epark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>阿蘇くじゅう国立公園</td>\n",
       "      <td>Aso Kuju National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>阿寒摩周国立公園</td>\n",
       "      <td>Akan Megumi National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>霧島錦江湾国立公園</td>\n",
       "      <td>Kirishima Jinjiang Bay National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>大山隠岐国立公園</td>\n",
       "      <td>Oyama Oki National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>伊勢志摩国立公園</td>\n",
       "      <td>Ise Shima National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>慶良間諸島国立公園</td>\n",
       "      <td>Kerama islands national park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>日光国立公園</td>\n",
       "      <td>Nikko National Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>十和田八幡平国立公園</td>\n",
       "      <td>Towada Hachimantai National Park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         park                                 epark\n",
       "0  阿蘇くじゅう国立公園                Aso Kuju National Park\n",
       "1    阿寒摩周国立公園             Akan Megumi National Park\n",
       "2   霧島錦江湾国立公園  Kirishima Jinjiang Bay National Park\n",
       "3    大山隠岐国立公園               Oyama Oki National Park\n",
       "4    伊勢志摩国立公園               Ise Shima National Park\n",
       "5   慶良間諸島国立公園          Kerama islands national park\n",
       "6      日光国立公園                   Nikko National Park\n",
       "7  十和田八幡平国立公園      Towada Hachimantai National Park"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:signate]",
   "language": "python",
   "name": "conda-env-signate-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
