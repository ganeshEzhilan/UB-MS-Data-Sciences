{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from google.cloud import translate\n",
    "\n",
    "path = '/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/Signate-88f03d8fec8b.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "translate_client = translate.Client()\n",
    "def translate_eng(text, target='en'):\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "    return result['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating training set\n",
    "train = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'train.tsv'.txt\")\n",
    "park_trans = pd.DataFrame()\n",
    "park_trans['park'] = list(set(train['park']))\n",
    "e_park = []\n",
    "for park in park_trans['park']:\n",
    "    if park.strip is not None:\n",
    "        try:\n",
    "            e_park.append(translator.translate(park).text)\n",
    "        except:\n",
    "            pass\n",
    "park_trans['epark'] = e_park\n",
    "t_train = train.merge(park_trans, on='park')[['datetime', 'epark', 'visitors']]\n",
    "t_train.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'ttrain.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating test set\n",
    "test = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'test.tsv'.txt\")\n",
    "t_test = test.merge(park_trans, on='park')[['index','datetime', 'epark',]]\n",
    "t_test = t_test.sort_values(by = ['index'])\n",
    "t_test.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'ttest.tsv'.txt\", sep='\\t', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating hotlink\n",
    "hotlink = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'hotlink.tsv'.txt\", sep='\\t')\n",
    "hotlink_trans = pd.DataFrame()\n",
    "hotlink_trans['keyword'] = list(set(hotlink['keyword']))\n",
    "e_keyword = []\n",
    "for word in hotlink_trans['keyword']:\n",
    "    if word.strip is not None:\n",
    "        try:\n",
    "            e_keyword.append(translator.translate(word).text)\n",
    "        except:\n",
    "            pass\n",
    "hotlink_trans['ekeyword'] = e_keyword\n",
    "t_hotlink = hotlink.merge(hotlink_trans, on='keyword')[['datetime', 'domain', 'ekeyword','count']]\n",
    "t_hotlink = t_hotlink.sort_values(by = ['datetime'])\n",
    "t_hotlink.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'thotlink.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating colopl\n",
    "colopl = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'colopl.tsv'.txt\", sep='\\t')\n",
    "colopl_trans = pd.DataFrame()\n",
    "colopl_trans['country_jp'] = list(set(colopl['country_jp']))\n",
    "e_country_jp = []\n",
    "for word in colopl_trans['country_jp']:\n",
    "    if word.strip is not None:\n",
    "        try:\n",
    "            e_country_jp.append(translator.translate(word).text)\n",
    "        except:\n",
    "            pass\n",
    "colopl_trans['ecountry_jp'] = e_country_jp\n",
    "t_colopl = colopl.merge(park_trans, on='park')[['year', 'month', 'epark','country_jp','count']]\n",
    "t_colopl = t_colopl.merge(colopl_trans, on='country_jp')[['year', 'month', 'epark','ecountry_jp','count']]\n",
    "t_colopl = t_colopl.sort_values(by = ['year','month'])\n",
    "t_colopl.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'tcolopl.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating weather\n",
    "weather = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'weather.tsv'.txt\", sep='\\t')\n",
    "\n",
    "#translating column names\n",
    "weatherCol_trans = pd.DataFrame()\n",
    "weatherCol_trans['col'] = list(weather.columns)\n",
    "e_wcol = []\n",
    "for col in weatherCol_trans['col']:\n",
    "    if col.strip is not None:\n",
    "        try:\n",
    "            e_wcol.append(translate_eng(col))\n",
    "        except:\n",
    "            pass\n",
    "weatherCol_trans['ecol'] = e_wcol\n",
    "t_weather = weather.copy()\n",
    "t_weather.columns = e_wcol\n",
    "\n",
    "\n",
    "#translating location \n",
    "weatherloc_trans = pd.DataFrame()\n",
    "weatherloc_trans['location'] = list(set(weather.iloc[:,1]))\n",
    "e_weatherloc = ['Towada', 'Oyama', 'Daejeon', 'Nikko', 'Tokashiki', 'Kumamoto', 'Kushiro', 'Aomori', 'Takamori', 'Toba', 'Kagoshima', 'Kazuno']\n",
    "\n",
    "weatherloc_trans['elocation'] = e_weatherloc\n",
    "t_weather = t_weather.merge(weatherloc_trans, on='location')\n",
    "t_weather = t_weather.drop(\"location\", axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#translating column 14-\"Weather overview (night: 18 o'clock - next day 06 o'clock)\"\n",
    "weatherover_trans = pd.DataFrame()\n",
    "weatherover_trans[\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\"] = list(set(weather.iloc[:,23]))\n",
    "e_weatherover = []\n",
    "for tmp in weatherover_trans[\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\"]:\n",
    "        try:\n",
    "            e_weatherover.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weatherover.append(\"\")\n",
    "weatherover_trans[\"eWeather overview (night: 18 o'clock - next day 06 o'clock)\"] = e_weatherover\n",
    "t_weather = t_weather.merge(weatherover_trans, on=\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\")\n",
    "t_weather = t_weather.drop(\"Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)\", axis = 1)\n",
    "\n",
    "#########\n",
    "weather_winddir = pd.DataFrame()\n",
    "weather_winddir[\"Most wind direction (16 directions)\"] = list(set(weather.iloc[:,14]))\n",
    "e_weather_winddir = []\n",
    "for tmp in weather_winddir[\"Most wind direction (16 directions)\"]:\n",
    "        try:\n",
    "            e_weather_winddir.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weather_winddir.append(\"\")\n",
    "weather_winddir[\"eMost wind direction (16 directions)\"] = e_weather_winddir\n",
    "t_weather = t_weather.merge(weather_winddir, on=\"Most wind direction (16 directions)\")\n",
    "t_weather = t_weather.drop([\"Most wind direction (16 directions)\"], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "weatherover_trans = pd.DataFrame()\n",
    "weatherover_trans[\"Weather overview (day: 06: 00-18: 00)\"] = list(set(weather.iloc[:,22]))\n",
    "e_weatherover = []\n",
    "for tmp in weatherover_trans[\"Weather overview (day: 06: 00-18: 00)\"]:\n",
    "        try:\n",
    "            e_weatherover.append(translate_eng(tmp))\n",
    "        except:\n",
    "            e_weatherover.append(\"\")\n",
    "weatherover_trans[\"eWeather overview (day: 06: 00-18: 00)\"] = e_weatherover\n",
    "t_weather = t_weather.merge(weatherover_trans, on=\"Weather overview (day: 06: 00-18: 00)\")\n",
    "t_weather = t_weather.drop(\"Weather overview (day: 06: 00-18: 00)\", axis = 1)\n",
    "t_weather.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'tweather.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating files in agoop folder\n",
    "#CITY_*.tsv\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "file_list = os.listdir(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop\")\n",
    "city_list = list(filter(lambda x: x.startswith('city'), file_list)) \n",
    "mesh_list = list(filter(lambda x: x.startswith('mesh'), file_list))\n",
    "month_list = list(filter(lambda x: x.startswith('month'), file_list))\n",
    "\n",
    "\n",
    "for file in city_list:\n",
    "    city = pd.read_table(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop/\",file), sep='\\t')\n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['prefecture'] = list(set(city.iloc[:,1]))\n",
    "    e_prefecture = []\n",
    "    for prefecture in tmpdf['prefecture']:\n",
    "        if prefecture is not None:\n",
    "            trans = translate_eng(prefecture)\n",
    "            re_comp = re.compile(re.escape('prefecture'), re.IGNORECASE)\n",
    "            trans = re_comp.sub(\"\",trans)\n",
    "            e_prefecture.append(trans.strip())\n",
    "    tmpdf['eprefecture'] = e_prefecture\n",
    "    t_city = city.merge(tmpdf, on='prefecture')\n",
    "    t_city = t_city.drop([\"prefecture\"], axis = 1)\n",
    "    \n",
    "    \n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['cwtv'] = list(set(city.iloc[:,3]))\n",
    "    e_cwtv = []\n",
    "    for cwtv in tmpdf['cwtv']:\n",
    "        if type(cwtv) is str:\n",
    "            trans = translate_eng(cwtv)\n",
    "            e_cwtv.append(trans.strip())\n",
    "        else:\n",
    "            e_cwtv.append(\"\")\n",
    "    tmpdf['ecwtv'] = e_cwtv\n",
    "    t_city = t_city.merge(tmpdf, on='cwtv')\n",
    "    t_city = t_city.drop([\"cwtv\"], axis = 1)\n",
    "    \n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['address'] = list(set(city.iloc[:,4]))\n",
    "    e_add = []\n",
    "    for add in tmpdf['address']:\n",
    "        if type(add) is str:\n",
    "            trans = translate_eng(add)\n",
    "            e_add.append(trans.strip())\n",
    "        else:\n",
    "            e_add.append(\"\")\n",
    "    tmpdf['eaddress'] = e_add\n",
    "    t_city = t_city.merge(tmpdf, on='address')\n",
    "    t_city = t_city.drop([\"address\"], axis = 1)\n",
    "    t_city.to_csv(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/tagoop/\",file), sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Translating Agoop/month** files\n",
    "for file in month_list:\n",
    "    month_time = pd.read_table(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/agoop/\",file), sep='\\t')\n",
    "    month_time = month_time.merge(park_trans, on='park')\n",
    "    month_time = month_time.drop([\"park\"], axis = 1)\n",
    "    month_time.to_csv(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/tagoop/\",file), sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Translating jorudan.tsv\n",
    "jorudan = pd.read_table(os.path.join(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/jorudan.tsv\"), sep='\\t')\n",
    "tjorudan = jorudan.merge(park_trans, on = \"park\")\n",
    "tjorudan = tjorudan.drop([\"park\"], axis = 1)\n",
    "\n",
    "           \n",
    "tmpdf = pd.DataFrame()\n",
    "tmpdf['departure_prefecture'] = list(set(city.iloc[:,1]))\n",
    "e_prefecture = []\n",
    "for prefecture in tmpdf['departure_prefecture']:\n",
    "    if prefecture is not None:\n",
    "        trans = translate_eng(prefecture)\n",
    "        re_comp = re.compile(re.escape('prefecture'), re.IGNORECASE)\n",
    "        trans = re_comp.sub(\"\",trans)\n",
    "        e_prefecture.append(trans.strip())\n",
    "tmpdf['edeparture_prefecture'] = e_prefecture\n",
    "tjorudan = tjorudan.merge(tmpdf, on='departure_prefecture')\n",
    "tjorudan = tjorudan.drop([\"departure_prefecture\"], axis = 1)\n",
    "\n",
    "tmpdf.columns = ['arrival_prefecture','earrival_prefecture']\n",
    "tjorudan = tjorudan.merge(tmpdf, on='arrival_prefecture')\n",
    "tjorudan = tjorudan.drop([\"arrival_prefecture\"], axis = 1)\n",
    "tjorudan.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/tjorudan.tsv\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Translating nied_oyama.tsv\n",
    "oyama = pd.read_table(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/'nied_oyama.tsv'.txt\", sep='\\t')\n",
    "\n",
    "#translating column names\n",
    "trans = pd.DataFrame()\n",
    "trans['col'] = list(oyama.columns)\n",
    "e_wcol = []\n",
    "for col in trans['col']:\n",
    "    if col.strip is not None:\n",
    "        try:\n",
    "            e_wcol.append(translate_eng(col))\n",
    "        except:\n",
    "            pass\n",
    "#trans['ecol'] = e_wcol\n",
    "oyama.columns = e_wcol\n",
    "oyama.to_csv(\"/Users/abhishekkumar/Documents/MS_UB/Fall_18/ESL506_StatisticalDataMiningI/Project/SIGNATE/translated/'tnied_oyama.tsv'.txt\", sep='\\t',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'location',\n",
       " 'Average temperature (℃)',\n",
       " 'Highest temperature (℃)',\n",
       " 'Lowest temperature (° C)',\n",
       " 'Total precipitation (mm)',\n",
       " '10 minutes Maximum precipitation (mm)',\n",
       " 'Daylight hours (hours)',\n",
       " 'Total total solar insolation (MJ / m2)',\n",
       " 'Deepest snow (cm)',\n",
       " 'Total snowfall (cm)',\n",
       " 'Average wind speed (m/s)',\n",
       " 'Maximum wind speed (m/s)',\n",
       " 'Maximum instantaneous wind speed (m/s)',\n",
       " 'Most wind direction (16 directions)',\n",
       " 'Average vapor pressure (hPa)',\n",
       " 'Average local atmospheric pressure (hPa)',\n",
       " 'Average humidity (%)',\n",
       " 'Average sea level pressure (hPa)',\n",
       " 'Minimum relative humidity (%)',\n",
       " 'Lowest sea level pressure (hPa)',\n",
       " 'Average cloud amount (10 points)',\n",
       " 'Weather overview (day: 06: 00-18: 00)',\n",
       " 'Weather overview (night: 18 o&#39;clock - next day 06 o&#39;clock)']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_wcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:signate]",
   "language": "python",
   "name": "conda-env-signate-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
