Introduction to Statistical Learning in R Lab 2 (Chapter 3)
========================================================

Load some R libs

```{r}
library(MASS)
library(ISLR)
lm.fit=lm(medv~lstat,data=Boston)
summary(lm.fit)
```

Show some QC figures on this lm fit to the data. As expected there is a very significant correlation between median value of homes in a neighborhood, and proportion of the population that is lower status. There is a lot of interesting stuff here. For example we see that there are some points with high leverage that are also nearly 3 on the residuals axis. **Not sure what absolute values of leverage are considered really high, I will need to look into the chapter more deeply for that. Looks like Cook's distance doesn't come into play, would those be the particularly dangerous points? Perhaps that is the key determinate.** From earlier in the chapter, just before the section on colinearity, it says that the average value of leverage is `(p+1)/n`. Also it always varies between 0 and 1. Figure 3.13 which shows an example of a dangerous situation, has a point with a leverage of a little over 0.25. In this case we are looking at one variable, so I think `p=1 n=506` either `r 2/506` or `r 3/506` should be the mean value of the leverage. We have a few points that are around 0.025 which is an order of magnitude higher than what we should expect for the mean, not sure how significant this is though. Here are some interesting online resources: http://www.statmethods.net/stats/rdiagnostics.html

```{r fig.width=11, fig.height=11}
par(mfrow=c(2,2))
plot(lm.fit)
```

And we can do a confidence interval on the coefficients. Remember that confidence intervals and prediction intervals are different. Prediction intervals as we will see are more conservative.

```{r}
confint(lm.fit)

predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
```

Here is a plot of the points, with the fitted line
```{r fig.width=7, fig.height=5}
plot(Boston$lstat,Boston$medv)
abline(lm.fit, lwd=3, col="red")
```

Plot of predicted fit vs residuals and studentized versions. Remember that the sutentized residuals are useful for determining outliers. Studentized residuals are devided by the standard error to make something like a Z score, so you can look at `sigma` levels. Values greater than 3 on studentized residuals are poential outliers. That corresponds to 3 `sigma` I think.

```{r fig.width=11, fig.height=5}
par(mfrow=c(1,2))
plot(predict(lm.fit), residuals(lm.fit), main="Residuals")
plot(predict(lm.fit), rstudent(lm.fit), main="Student Fit")
```

There is some evidence of non-linearity based on the resudials plot. Lets look into the leverage statistic using the hatvals function.

```{r fig.width=7, fig.height=5}
plot(hatvalues(lm.fit))
```

To see which index point has the max leverage, we can use which.max to return the index of the max value.
```{r}
which.max(hatvalues(lm.fit))
```

Multiple Linear Regression
--------------------------
```{r}
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

We can also look at all variables at the same time with this shorthand syntax.
```{r}
lm.fit = lm(medv~., data=Boston)
summary(lm.fit)
```

Interesting that when we include all variables, age, which used to be significant, is no longer called that way.

One cool thing is that we can access certain elements of the lm summary like so:
```{r}
summary(lm.fit)$r.sq
```

Also the vif function from the car package can calcualte soemthing called the "variance inflation" factor. **How do we know what a good vs bad inflation factor is?**
```{r}
library(car)
vif(lm.fit)
```

Let's try a regression excluding one variable. There are two ways to do this, we can specify a new regression with the special -age syntax after the . syntax, otherwise we can use the "update" method to update the previous fit removing the age variable. We can remove multiple variables with this syntax as well.

```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit2=update(lm.fit, ~.-age)
summary(lm.fit2)
lm.fit3=lm(medv~.-age-indus,data=Boston)
summary(lm.fit3)
```

And here is a plot including the significant variables identified previously (-age,-industry)
```{r fig.width=11, fig.height=11}
par(mfrow=c(2,2))
plot(lm.fit3)
```


Interaction Terms
----------------
When we use `lstat*age` in the formula, the individual terms `lstat` and `age` are automatically included. So for example we can do this to explore the version of lstat and age as interactive terms.

```{r}
summary(lm(medv~lstat*age,data=Boston))

```

Non-linear transformations of predictors
-------------------
We can look at things like something squared using the `I()` function which helps when you want to use a special symbol in your equation.
```{r}
lm.fit2=lm(medv~lstat+I(lstat^2), data=Boston)
summary(lm.fit2)
```

We can use the `anova()` function to further quantify how much better the quadratic fit is superior to the linear fit.
```{r}
lm.fit=lm(medv~lstat,data=Boston)
anova(lm.fit,lm.fit2)
```

And here is a plot of the fits for lstat^2
```{r fig.width=11, fig.height=11}
par(mfrow=c(2,2))
plot(lm.fit2)
```

We can also fit higher order polynomials using the `poly()` function, which does the work of writing out all of the decreasing polynomial terms for us given a variable, and the numbers of polynomials you want to fit. We could also do something like a log transform in the linear model

```{r}
lm.fit5=lm(medv~poly(lstat,5),data=Boston)
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))
```

Qualitative Predictors
------------------

R actually automatically will create dummy variables when you have a predictor. We also add a few specific interaction terms to the full model, namely Income and Advertising, along with Price and Age.

```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
```

We can see what the dummy coding is for a variable using the `contrasts()` function.

```{r}
contrasts(Carseats$ShelveLoc)
```

Cool! contrasts can be set for any factor, you can use the above function to specify what the contrasts are for a given factor in a dataframe. This sounds very handy.

So to interpret the output of the `lm` above with interpreting the dummy variables, keep in mind that `ShelveLocGood` being a positive and significant association represents an improvement over the default case, `ShelveLocBad`, similarly `ShelveLocMedium` represents a slightly less, yet still positive and significant improvement.


```{r}
LoadLibraries <- function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibraries()
```

